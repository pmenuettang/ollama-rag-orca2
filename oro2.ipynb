{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.4-cp311-cp311-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install langchain\n",
    "# %pip install pypdf\n",
    "# %pip install rapidocr-onnxruntime\n",
    "# %pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"sample_data/Orca 2.pdf\", extract_images=True)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"orca2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = FAISS.from_documents(pages, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"search_secure_by_design\",\n",
    "    \"Searches and returns informations regarding the state of the art of security by design.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOllama(\n",
    "    model=\"orca2\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"[INST] <<SYS>> Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. <</SYS>>\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:[/INST]\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    chat_model,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am an AI assistant that can help you find information. You have asked me to summarize a document about a cautious reasoner model called Orca 2. Here is a possible summary:\n",
      "\n",
      "Key points:\n",
      "- The document describes the data and evaluation methods for a new cautious reasoner model called Orca 2, which can learn from prompt erasing and few shot data.\n",
      "- The document explains how the model can generate text for various tasks that require reasoning, knowledge, language understanding, summarization, safety, bias, and comprehension.\n",
      "- The document provides examples of texts generated by the model and compares them with human references and other models.\n",
      "\n",
      "Summary:\n",
      "The document introduces Orca 2, a new cautious reasoner model that can learn from different sources of data. The model can generate text for various tasks that test its abilities to reason, understand language, answer questions, summarize information, avoid errors, handle bias, and comprehend texts. The document shows some examples of the model's outputs and how they compare with human references and other models."
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Give me a summary of this document.\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To summarize, the explanation provided by the user is as follows:\n",
      "\n",
      "- The user's question is about how the model can generate text that requires reasoning.\n",
      "- The user seems to be asking for a summary of the explanation given earlier in the conversation.\n",
      "- The explanation involves the use of WizardLM, an instruction-tuned version of LLaMA 2, which can generate diverse and complex texts through autonomous instructions.\n",
      "- Additionally, other models like Orca 1 and GPT Models are used for comparison purposes.\n",
      "- Benchmarks such as AGIEval, DROP, CRASS, RACE, BBH, and GSM8K are mentioned to assess the reasoning capabilities of LLMs.\n",
      "- The user also seems interested in understanding how LLMs can understand language, knowledge, and perform multi-step reasoning tasks through MMLU, ARC, and Big-Bench Hard."
     ]
    }
   ],
   "source": [
    "question = \"Give me a summary of the explanation about how the model can generate text that require reasoning.\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
